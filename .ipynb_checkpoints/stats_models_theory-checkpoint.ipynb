{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Generating Processes and Statistical Models\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "In the case of unsupervised learning, we have a data generating process described by a random variable $X$. $X$ is \n",
    "\n",
    "$$ Z: \\Omega \\rightarrow ℝ^n $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ Z: \\Omega \\rightarrow ℕ^n $$\n",
    "\n",
    "In either case, $\\mathcal{A}$ is the corresponding $\\sigma$-algebra over $\\Omega$ and $P$ the corresponding probability measure on $\\mathcal{A}$. $Z$ induces a probability measure $P'$ on $\\mathcal{B}(ℝ^n)$, the Borel $\\sigma$-algebra, in the case of $Z: \\Omega \\rightarrow ℝ^n$ or on $\\mathcal{P}(ℕ^n)$, the power set of $ℕ^n$, in the case of $Z: \\Omega \\rightarrow ℕ^n $. In both cases, we assume that $P'$ is parametrized by $\\theta \\in \\Theta$ and we have a statistical model $\\mathbf{P} = \\{P_{\\theta} | \\theta \\in \\Theta\\}$. Given data from the data generating process, we want to estimate $\\theta$.\n",
    "\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "In the case of supervised learning, we have random variables $X$ and $Y$ with a joint distribution $Z$, which represents the whole data generating process. $X$ generally represents the independent variables and $Y$ represents the dependent variable.\n",
    "\n",
    "$$ X: \\Omega \\rightarrow \\alpha $$\n",
    "$$ Y: \\Omega \\rightarrow \\beta $$\n",
    "$$ Z: \\Omega \\rightarrow \\alpha \\times \\beta $$\n",
    "\n",
    "$\\mathcal{A}$ is the corresponding $\\sigma$-algebra over $\\Omega$ and $P$ the corresponding probability measure on $\\mathcal{A}$. $\\alpha$ is usually a finite subset of $ℕ$, $ℕ$ itself, $ℝ$ or any direct product of the three and represents the values of the independent variables. $\\beta$ is usually a finite subset of $ℕ$, $ℕ$ itself or $ℝ$ and represents the values of the dependent variable. $Z$ induces a probability measure $P'$ on the $\\sigma$-algebra defined over $\\alpha \\times \\beta$. \n",
    "\n",
    "When modeling supervised learning problems, we assume some functional relationship between $X$ and $Y$ in the joint distribution $P'$ in the form of \n",
    "\n",
    "$$ P'(Y | X = x) \\sim f_{\\theta_1}(x) + n_{\\theta_2}(\\omega) $$\n",
    "\n",
    "for regression problems and \n",
    "\n",
    "$$ P'(Y | X = x) \\sim g(f_{\\theta}(x) + n_{\\theta_2}(\\omega)) $$\n",
    "\n",
    "for classification problems.\n",
    "\n",
    "We model this functional relationship between $X$ and $Y$ through a deterministic function $f_{\\theta_1}$, which is parametrized by $\\theta_1 \\in \\Theta_1$ and a noise term $n_{\\theta_2}$, which adds randomness to $P'(Y | X = x)$ and is parametrized by $\\theta_2 \\in \\Theta_2$. \n",
    "\n",
    "This functional relationship partly defines the joint distribution $P'$ over $X$ and $Y$. For fully defining the joint distribution $Z$ we would also need to define the marginal distribution over $X$. This, however, we rarely do in the supervised learning setting (see bellow).\n",
    "\n",
    "\n",
    "### Regression\n",
    "\n",
    "Let's assume that we are in the regression setting. This is, $\\beta = ℝ$. We assume a functional relationship between $X$ and $Y$ of the form\n",
    "\n",
    "$$ P'(Y | X = x) \\sim f_{\\theta_1}(x) + n_{\\theta_2}(\\omega), \\forall x $$\n",
    "\n",
    "Usually we additionally assume that $E[n_{\\theta_2}(\\omega)] = 0$, which leads to $E[Y | X = x] = ax + b$ for all $x$. This implies the following property of $f_{\\theta_1}$\n",
    "\n",
    "$$ f_{\\theta_1}: \\alpha \\rightarrow ℝ; x \\rightarrow f_{\\theta_1}(x) = ax + b = E[Y | X = x], \\forall x $$\n",
    "\n",
    "Given $n$ independent and identically drawn samples $(z)_{i = 1}^n = (x, y)_{i=1}^n$ from the data generating process, we want to infer $\\theta = (\\theta_1, \\theta_2) \\in \\Theta$.\n",
    "\n",
    "\n",
    "### Classification\n",
    "\n",
    "Let's assume that we are in a finite classification setting. This is, $\\beta$ is $\\{1, ..., m\\}$, a finite subset of $ℕ$ where $m$ is the number of classes we want to distinguish. We assume a functional relationship between $X$ and $Y$ of the form\n",
    "\n",
    "$$ P'(Y | X = x) = P'((Y' | X = x) \\circ g) $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ (Y' | X = x): \\Omega \\rightarrow ℝ^m; \\omega \\rightarrow s = f_{\\theta_1}(x) + n_{\\theta_2}(\\omega)$$\n",
    "\n",
    "$$ g: ℝ^m \\rightarrow \\{1, ..., m\\}; s \\rightarrow y = g(s) $$\n",
    "\n",
    "So\n",
    "\n",
    "$$ (Y | X = x): \\Omega \\rightarrow \\{1, ..., m\\}; \\omega \\rightarrow y = g(f_{\\theta_1}(x) + n_{\\theta2}(\\omega)) $$\n",
    "\n",
    "\n",
    "$P'(Y | X = x)$ is a distribution over the finite set $\\{1, ..., m\\}$ which assigns a probability $P'(Y = c_i | X = x)$ for each class $c_i$. The formulation of the relationship between $X$ and $Y$ is more complicated than in the regression case from above. This is because $X$ takes values in ℝ and we have to somehow transform those continuous values in ℝ to a discrete distribution over $\\{1, ..., m\\}$. This makes splitting $(Y | X = x)$ into the somewhat cumbersome $(Y' | X = x)$ and $g$ necessary.\n",
    "\n",
    "From now on, we will identify the probability measure $P'$ with it's Radon-Nikodym derivative $p'$. This is, with its probability density function (pdf) and call it just $p$ for simplicity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
