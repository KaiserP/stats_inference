{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bayesian Monte Carlo Parameter Inference\n",
    "\n",
    "In Bayesian inference statistic we want to find the distribution of $\\theta$ given data $z_{i=1}^n$. This is, we want to find \n",
    "\n",
    "$$ p(\\theta | z_{i=1}^n) $$ \n",
    "\n",
    "When using Monte Carlo Bayesian parameter inference, we sample from $p(\\theta | z_{i=1}^n)$. The random samples we draw from $p(\\theta | z_{i=1}^n)$ allow us to calculate estimates of expected value, mode, variance, quantiles, marginal distributions, and other summary statistics of $\\theta$. \n",
    "\n",
    "This sampling approach is very different from the approach where we try to determine those quantities analytically from $p(\\theta | z_{i=1}^n)$. Analytical derivation of those quantities is generally only possible if $p(\\theta | z_{i=1}^n)$ is of a relatively simple form. In the case of Bayesian parameter inference in more complicated statistical and machine learning models $p(\\theta | z_{i=1}^n)$ is generally no of a simple form and we need to use sampling methods to get summary statistics of $p(\\theta | z_{i=1}^n)$. \n",
    "\n",
    "To sample from $p(\\theta | z_{i = 1}^n) $ we need an expression for it. Since we are in the Bayesian inference setting we have \n",
    "\n",
    "$$ p(\\theta | z_{i = 1}^n) = \\frac{p(z_{i = 1}^n | \\theta)p(\\theta)}{p(z_{i = 1}^n)} $$\n",
    "\n",
    "And since we are going to use the Metropolis-Hastings sampling algorithm it's enough to have\n",
    "\n",
    "$$ p(\\theta | z_{i = 1}^n) \\propto p(z_{i = 1}^n | \\theta)p(\\theta) $$\n",
    "\n",
    "\n",
    "Let's first look at the marginal distribution for $\\theta$ \n",
    "\n",
    "$$ p(\\theta) $$\n",
    "\n",
    "We have to specify this for our parameter $\\theta = (a, b, \\sigma^2)$ in order to sample from $p(\\theta | x_{i=1}^n)$. This is the prior knowledge about our data generating process, which is a component that is mandatory to specify in the Bayesian inference setting. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "Let's first look at \n",
    "\n",
    "$$ p(z_{i = 1}^n | \\theta) = p((x, y)_{i = 1}^n | \\theta) $$ \n",
    "\n",
    "first. The parameter $\\theta = (\\theta_1, \\theta_2)$ describes the conditional distribution of $Y$ given $X = x$. This means, that we partly describe the joint distribution $Z = (X, Y)$ through the conditional distribution $Y | X =x $. The parameter $\\theta$, however, doesn't say anything about the distribution of $X$. To fully describe $p(z_{i = 1}^n | \\theta) = p((x, y)_{i = 1}^n | \\theta)$ we need the marginal distribution of $X$ under $\\theta$. This is, $p(x_{i=1}^n | \\theta$). Then we can describe $p(z_{i = 1}^n | \\theta)$ using the chain rule of probability like\n",
    "\n",
    "$$ p(z_{i = 1}^n | \\theta) = p((x,y)_{i = 1}^n | \\theta) = p(y_{i = 1}^n | x_{i = 1}^n, \\theta) p(x_{i=1}^n | \\theta) $$\n",
    "\n",
    "However, often in the supervised learning setting we assume that $X$, which describes the independent variables, is actually not random. This is, we set\n",
    "\n",
    "$$ p(z_{i = 1}^n | \\theta) = p(y_{i = 1}^n | \\theta; x_{i=1}^n) $$\n",
    "\n",
    "And consequently we sample from\n",
    "\n",
    "$$ p(\\theta | y_{i = 1}^n; x_{i=1}^n) \\propto p(y_{i = 1}^n | \\theta; x_{i=1}^n)p(\\theta) $$\n",
    "\n",
    "From a technical point of view, treating $X$ as deterministic makes our life easier because we have one level of randomness less, which we need to take into consideration. Treating $X$ as random might make sense if we have some justified assumption about the distribution of $X$. For the examples here, and probably for most applications of Bayesian parameter inference in supervised learning settings, $X$ is treated as deterministic. I could imagine that this approach leads to some some kind of underestimation of uncertainty in the Bayesian inference setting. How severe that is is difficult for me to say at this point. \n",
    "\n",
    "Note that treating $X$ as uniformly distributed over some interval $[c, d] \\in ‚Ñù$ and calculating the marginal distribution $p(y_{i=1}^n | \\theta)$ wouldn't work because we  would have \n",
    "\n",
    "$$ p(y_{i=1}^n | \\theta) = \\int_c^d{p(y_{i=1}^n | x_{i=1}^n, \\theta) p(x_{i=1}^n | \\theta)}dx = \\int_c^d{p(y_{i=1}^n | x_{i=1}^n, \\theta)}dx = const + n_{\\theta_2}(\\omega) $$\n",
    "\n",
    "This is, integrating out $x_{i=1}^n$ would leave us with the marginal distribution for $y_{i=1}^n$ in which the functional relationship between $X$ and $Y$ would have gotten lost and we would only be left with the random component $n_{\\theta_2}(\\omega)$.\n",
    "\n",
    "\n",
    "## Statistical Inference Alternatives\n",
    "\n",
    "Note that we could also use a different statistical inference approach to estimate $\\theta$, like e.g. maximum likelihood estimation. Remember the difference between $\\hat{\\theta}_{MAP}$ and $\\hat{\\theta}_{MLE}$.\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\underset{\\theta\\ \\in \\Theta}{argmax}{p(\\theta | x_{i=1}^n)} $$\n",
    "\n",
    "$$ \\hat{\\theta}_{MLE} = \\underset{\\theta\\ \\in \\Theta}{argmax}{L(x_{i=1}^n; \\theta)} $$\n",
    "\n",
    "$L$ is the likelihood function. In the case of maximum likelihood inference, $L$ is not a probability distribution and thus we can't use random sampling to infer parameters in the Bayesian inference setting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}